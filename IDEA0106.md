#  **动机&创新 稿3** 2026.01.06

------

### 1. 引言与动机 (Introduction & Motivation)

`BaseLine (MOSE) 的局限:`

近期在线持续学习（OCL）领域的进展，特别是多层在线序列专家（MOSE），通过引入空间层级的专家混合架构（Mixture-of-Experts）和反向自蒸馏机制（RSD），有效地利用了深层网络的语义表征能力，显著缓解了在线持续学习（OCL）中的“过拟合-欠拟合”困境。MOSE 本质上是一种**空间维度**的解决方案，通过模型结构的模块化来平衡新旧知识的获取。

然而，我们指出 MOSE 在**时间维度**上的样本利用效率存在显著的次优性。在 OCL 严格的单遍数据流约束下，样本的学习状态随时间呈现非均匀分布，而 MOSE 采用的均匀随机采样策略忽视了样本在不同学习阶段的**动态需求**：

**近期难样本的优化迟滞：** 部分样本由于视图差异大或特征复杂，表现出高不确定性，这些样本通常需要“短期、高频”的复述才能被巩固（类似于生物学中的短期可塑性），而随机采样无法提供这种聚焦性的训练强度。*(验证见附录1)*

**回放资源的静态错配：** 在训练的不同阶段，模型面临的主要矛盾在变化。有时需要集中攻克难例（高塑性需求），有时则需要广泛回顾旧识以防遗忘（高稳定性需求）。MOSE 缺乏感知这种状态变化的机制，导致在模型需要巩固难样本时预算被分散，而在模型需要稳定复习时预算又被浪费在震荡样本上。*(验证见附录2)*

`本文方法 (Our Approach):`

我们的**动态双时间尺度回放（Dynamic Temporal-Structural Replay, DTSR）框架** ，与专注于内部一致性的MOSE不同，DTSR显式地对数据回放的时间调度进行建模，将短期不确定性巩固与长期记忆保持解耦。我们提出了一种**不确定性驱动的动态调度策略**，利用特征的一致性实时评估样本状态，并智能切换回放源，从而在不增加计算成本的前提下最大化回放效率。最后，我们通过时间稳定性约束增强了 MOSE 的内部蒸馏，将当前学习锚定到上一个任务的决策边界。

------

### 2. 贡献 (Contributions)
- `1. 架构创新：双时间尺度回放调度 `

  受脑科学中海马体（短期高塑性）和内皮层（长期高稳定）协作的启发，为了模拟记忆巩固过程中不同阶段的需求，我们提出了一种解耦的回放架构，划分短期不确定性缓冲区(U-buffer)与长期主缓冲区（Main Buffer）。

  - **机制描述：** 该架构基于样本的实时学习状态动态分配计算资源。U-Buffer 作为一个**高通滤波器**，专门捕获并高频回放那些以高视图不一致性分数为特征的“未收敛样本”。一旦样本的不确定性降低，它们将不再占据短期资源，而是通过主缓冲区进行长期的分布维护。
  - **一致性约束：** 针对 U-Buffer 中高频回放可能导致的特定视图过拟合问题，我们引入了**视图一致性约束**，强制模型学习对数据增强鲁棒的不变性特征。


- ` 2. 策略创新：不确定性评估与动态双路径回放`

  在双时间尺度架构之上，我们设计了一套动态调度算法，以最大化单位回放预算的梯度收益：

  - **不确定性评估指标：** 我们利用同一样本在不同增强视图下特征表示的余弦距离作为衡量不确定性的代理指标。该指标直接指导 U-Buffer 的写入，确保计算资源集中于当前处于“震荡期”的样本。
  - **动态双路径切换：** 我们摒弃了静态的混合采样，设计了**不确定性感知切换策略**。模型实时监测两个缓冲区的平均不确定性水平：当 U-Buffer 不确定性更高时，回放预算全额投入到难样本的快速巩固（Path 1）；当主缓冲区不确定性更高时，回放预算则转向对历史分布的广泛回顾（Path 2）。这种动态切换确保了模型始终在处理当前最需要被处理的数据。


- `3. 正则化创新：时间稳定性蒸馏 (Temporal Stability Distillation, TSD)`

  为了弥补 MOSE 仅关注当前模型内部一致性的不足，我们引入了跨任务周期的**时间锚定机制**。

  - **方法描述：** 利用上一任务结束时的冻结模型（Teacher）指导当前模型（Student）。我们在特征层施加余弦相似度约束以维持流形结构的稳定性，并在 Logit 层施加**新类掩码 KL 散度**。
  - **作用：** 这种双重蒸馏策略有效抑制了特征空间随任务序列的漂移，确保新知识的嵌入不会以破坏旧知识的表征结构为代价。

------

### 3. 方法论述的形式化描述 (Methodology)

#### 3.1. 双时间尺度回放调度

为了在时间维度上区分样本的学习状态，我们根据特征对数据增强的敏感性来量化“学习难度”。对于样本 $x$，令 $f_\theta(x)$ 为其特征表示，$\text{Aug}(x)$ 为其增强视图。我们定义不确定性指标 $u(x)$ 为两者在特征空间中的余弦距离：

$$u(x) = 1 - \text{cos}(f_\theta(x), f_\theta(\text{Aug}(x)))$$

为了筛选出处于剧烈变化期的样本，我们维护当前批次不确定性的统计量（均值 $\mu_{u}$ 与标准差 $\sigma_{u}$）。仅当 $u(x) > \mu_{u} + \sigma_{u}$ 时，该样本被判定为“高不确定性”并被存入 U-Buffer。

#### 3.2. 动态双路径回放策略

在训练过程中，我们根据两个缓冲区的平均不确定性水平（$\bar{u}_U​$ 与 $\bar{u}_M​$）动态切换回放模式，以实现“塑性-稳定性”的动态平衡。回放批次 $\mathcal{B}_{replay}​$ 的组成遵循以下**双路径切换逻辑**：

$$\mathcal{B}_{replay} \sim \begin{cases} \mathcal{D}_{U}, & \text{if } \bar{u}_U > \bar{u}_M \quad (\text{Path 1: Uncertainty Consolidation}) \\\mathcal{D}_{M}^{rand}, & \text{if } \bar{u}_M \ge \bar{u}_U \quad (\text{Path 2: Stability Maintenance})\end{cases}$$ 

- **Path 1 (不确定性巩固)**: 当 U-Buffer 的不确定性主导时，表明当前存在大量未收敛的难样本。此时，我们从 U-Buffer 采样并施加视图一致性正则化，将高频回放转化为稳定的特征优化：

  $$\mathcal{L}_{view} = \mathbb{E}_{x \sim \mathcal{B}_{replay}} || f_\theta(x) - f_\theta(\text{Aug}(x)) ||^2_2$$



- **Path 2 (稳定性维护):** 当主缓冲区的不确定性相对较高（或 U-Buffer 为空）时，表明难样本已初步收敛。此时，我们切换至从主缓冲区进行**随机采样（Random Sampling）**。这确保了模型能够广泛地回顾历史分布，防止在长时间序列训练中遗忘那些已掌握的低不确定性旧知识。

#### 3.3. 时间稳定性蒸馏 (TSD:Temporal Stability Distillation)

为了防止模型在长时间序列训练中发生流形漂移，我们引入上一任务结束时的冻结模型 $f_{\theta_{t-1}}$ 作为时间锚点。我们采用特征级与 Logit 级双重蒸馏：

$$\mathcal{L}_{TSD} = \lambda_{feat} \underbrace{\mathbb{E}[1 - \text{cos}(f_\theta(x), f_{\theta_{t-1}}(x))]}_{\text{Feature Stability}} + \lambda_{kd} \underbrace{\text{KL}(\sigma(\mathbf{z}_{old} / T) || \sigma(\mathbf{z}^{t-1}_{old} / T))}_{\text{Logit Stability}}$$

特别地，在计算 Logit 蒸馏时，我们对输出应用**新类掩码（New-Class Masking）**，仅计算旧类别索引集合 $\mathcal{C}_{old}$ 上的 KL 散度。这消除了新任务未初始化 Logits 的噪声干扰，确保了知识传递的准确性。

------

### 4. 为什么这比 MOSE 更好？

**动态 vs. 静态**：MOSE 假设所有缓冲样本是平等的，采用静态的均匀采样。我们证明了样本具有不同的学习生命周期。通过区别对待高不确定性样本（U-Buffer），我们加速了难数据的收敛，这对 OCL 有限的 epoch 设置至关重要。

**自适应 vs. 固定**：MOSE 的回放策略是固定的，而我们的双路径策略能够根据模型当前的训练状态（即两个 Buffer 的相对不确定性）**自适应**地调整关注点——是在攻克难关还是在巩固基础。这种机制赋予了数据流更高的智能性。

**完备性：** 补全了 MOSE 缺失的“跨时间维度”约束（TSD），与 MOSE 原有的“跨空间层级”约束（RSD）构成了完整的时空正则化闭环。

### 附录1. 近期难样本的优化迟滞

**论点：** MOSE 无法为处于“震荡期”的难样本提供必要的高频复述。

- **原文方法**：

  结合论文 Sec 5.1 Implementation Details (Page 6) 中描述:"MOSE applies reservoir sampling strategy [58] and random memory update."

  这证实了 MOSE 采用的是均匀随机采样。



- **推导**：

  设样本 $x_{easy}$ 为简单样本（特征稳定），$x_{hard}$ 为非稳态难样本（特征震荡剧烈）。

  在均匀随机采样下，任意样本被回放的期望概率 $P(retrieval)$ 仅取决于缓冲区大小 $M$ 和当前样本总数 $N$，即 $P \approx M/N$。

  **结论：** $P(x_{easy}) = P(x_{hard})$。

  然而，根据梯度下降原理，非稳态样本（High Loss/Gradient Norm）需要更多的梯度更新步数（即更高的 $P$）才能收敛。MOSE 给予它们与简单样本相同的关注度，必然导致难样本在有限的 OCL 窗口期内**梯度累积不足**。

### 附录2. 回放资源的静态错配

**论点：** MOSE 缺乏感知状态变化的机制，导致预算分配与模型当前的训练需求(塑性/稳定)不匹配。

**论据：**

- **训练初期：** 模型面对新任务的冲击，此时 U-Buffer（如果存在）中的不确定性会激增，模型亟需攻克新特征（需要高可塑性）。但 MOSE 此时仍在按固定比例回放旧数据，导致新任务学习被稀释。
- **训练后期：** 模型已基本掌握新任务，此时应重点防止遗忘（需要高稳定性）。但 MOSE 无法感知这种收敛状态（没有我们方法里面双路径的IF语句），可能仍在通过数据增强对已收敛样本进行不必要的训练，或未能增加对旧知识的回顾比重。
- **结论：** MOSE 的资源分配是**静态的**，而模型的训练状态是**动态的**。

